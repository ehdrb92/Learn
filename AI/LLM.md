## 인공신경망의 동작원리

딥러닝은 인공 신경망이라는 알고리즘을 대표하는 용어로 같은 의미로 생각할 수 있다.

딥러닝 알고리즘에는 피드 포워드 신경망(Feed forward Neural Network, FNN), 합성곱 신경망(Convolution Neural Network, CNN), 순환 신경망(Recurrent Neural Network) 등이 존재한다. 오토인코더(autoencoder), 트랜스포머(transformer)와 같이 신경망이라는 이름이 붙지 않은 것도 있다.

- 경사 하강법(Gradient descent): 딥러닝 알고리즘을 학습시키는 방법으로 점진적으로 정답에 가까운 값을 찾아가는 기법이다.
- 임베딩(Embedding): 배열(임베딩 벡터)에 무작위의 실수를 담아서 인간 언어를 표현하는 방식 각 임베딩 벡터의 거리를 통해 단어 사이의 연관관계를 파악한다.
- 인코더와 디코더: 인코더는 입력된 데이터를 받아들여서 응답할 데이터를 반환하고 디코더가 반환된 데이터를 해석해서 다시 전달한다.


## LLM

대규모 언어 모델(Large Language Model, LLM)은 학습된 데이터를 기반으로 인간의 언어와 유사한 텍스트를 이해(자연어 처리, Natural Language Process, NLP)하고 생성하도록 설계된 복잡한 시스템. 트랜스포머라는 구조를 기반으로 하는 인공지능의 일종이다. 문장이나 단락에서 단어의 위치에 관계없이 단어의 중요도를 평가하여 텍스트를 처리하는 주의력이라는 메커니즘을 사용

일반적으로 수 백만 개 이상의 매개변수를 가지고 있으며(예로 GPT3는 1750억개), 이를 통해 다양한 언어 패턴과 구조를 학습할 수 있다.

LLM을 만들기 위해서는 대규모 텍스트 데이터, 엄청난 연산량의 컴퓨팅 리소스가 필요하다.

대부분의 LLM은 트랜스포머 딥러닝 모델을 사용한다.

어떻게 동작할까?

1. Input and Tokenization: 입력 텍스트를 토큰이라고 하는 관리 가능한 조각으로 분리한다. 토큰은 단어, 단어의 일부 혹은 구두점일 수 있다.
2. Embedding: 각 토큰은 임베딩이라는 숫자 형식으로 변환된다. 임베딩은 토큰의 신원과 학습 데이터를 기반으로 한 문백상의 뉘앙스를 포착한다.
3. Transformer Architecture: LLM의 핵심은 여러 계층의 자기 주의 메커니즘을 사용하는 트랜스포머 구조이다. 계층을 통해 모델은 각 토큰의 출력을 예측할 때 입력 시퀀스의 다른 부분에 집중 할 수 있다.
4. Decoder and Output: 마지막에는 처리된 임베딩을 디코딩하여 텍스트를 생성한다. 모델은 한 번에 하나의 토큰을 예측하며, 이는 다음 토큰을 예측하기 위한 입력의 일부로 모델에 다시 피드백된다.

LLM은 텍스트 자동 완성, 문법 검사, 코드 작성, 음악 작곡, 대화 시뮬레이션, 콘텐츠 생성 등 폭넓게 활용될 수 있다.